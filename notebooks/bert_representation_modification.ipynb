{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Overview"
      ],
      "metadata": {
        "id": "3OvZfKYGSSVa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The goal of this project is to modify the output of a specific layer in a BERT model by integrating additional information through various methods. The solution should be flexible, allowing changes to the layer being modified and the method of integration.\n",
        "\n",
        "The project provides two primary approaches for integrating additional information into the BERT model:\n",
        "\n",
        "1.   **Hook-Based Approach**: Uses PyTorch hooks to modify the input of a specific layer during the forward pass.\n",
        "2.   **Custom Layer-Based Approach**: Defines a custom layer that integrates additional information.\n",
        "\n",
        "Both approaches enable flexible manipulation of BERT's internal representations for various layers and integration methods.\n",
        "\n",
        "Both the hook-based method and the custom layer-based method resulted in the **same outcome**. When tested, both methods produced identical modified hidden states for the specific layer in the BERT model. In terms of runtime, the **hook-based approach was slightly faster**."
      ],
      "metadata": {
        "id": "1hplvHATbvSg"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Gz1E_NhM4aa"
      },
      "source": [
        "# Dependecies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "QoYUCxR2IVcG"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from transformers import BertModel, BertTokenizer\n",
        "import numpy as np\n",
        "import random\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DR142SFMJA2X"
      },
      "source": [
        "# Common Setup Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ktrThlBFJBGl"
      },
      "outputs": [],
      "source": [
        "def set_seed():\n",
        "    # Set the random seed for reproducibility.\n",
        "    seed = 42\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "def setup_model_and_tokenizer(model_name='bert-base-uncased'):\n",
        "    # Load a pre-trained BERT model and tokenizer.\n",
        "    model = BertModel.from_pretrained(model_name)\n",
        "    tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "    return model, tokenizer\n",
        "\n",
        "def prepare_inputs(text, tokenizer):\n",
        "    # Tokenize input text and convert to tensors.\n",
        "    return tokenizer(text, return_tensors='pt')\n",
        "\n",
        "def create_additional_input_vector(hidden_size):\n",
        "    # Create a random tensor of specified hidden size.\n",
        "    set_seed()\n",
        "    return torch.randn(1, 1, hidden_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7NybcvAHoub"
      },
      "source": [
        "# Common Integration Method Apllier\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "hqhdm7g-I96m"
      },
      "outputs": [],
      "source": [
        "class IntegrationMethodApplier(nn.Module):\n",
        "    def __init__(self, integration_method):\n",
        "        \"\"\"\n",
        "        Initialize the IntegrationMethodApplier with a specified method.\n",
        "\n",
        "        Parameters:\n",
        "        integration_method (str): The method to integrate additional input ('addition' or 'multiplication').\n",
        "        \"\"\"\n",
        "        super(IntegrationMethodApplier, self).__init__()\n",
        "        self.integration_method = integration_method\n",
        "\n",
        "    def forward(self, input_tensor, additional_input_vector):\n",
        "        \"\"\"\n",
        "        Apply the specified integration method to the input tensor and additional input vector.\n",
        "\n",
        "        Parameters:\n",
        "        input_tensor (torch.Tensor): The original input tensor.\n",
        "        additional_input_vector (torch.Tensor): The additional input vector to integrate.\n",
        "\n",
        "        Returns:\n",
        "        torch.Tensor: The result of the integration.\n",
        "        \"\"\"\n",
        "        if self.integration_method == \"addition\":\n",
        "            return input_tensor + additional_input_vector\n",
        "        elif self.integration_method == \"multiplication\":\n",
        "            return input_tensor * additional_input_vector\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported integration method\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SpdvxrQxH8Bj"
      },
      "source": [
        "# Hook-Based Approach"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "JlEALueliFas"
      },
      "outputs": [],
      "source": [
        "class HookBasedBERTModifier:\n",
        "    def __init__(self, model, layer_number, integration_method_applier):\n",
        "        \"\"\"\n",
        "        Initialize the HookBasedBERTModifier.\n",
        "\n",
        "        Parameters:\n",
        "        model (BertModel): The BERT model to modify.\n",
        "        layer_number (int): The layer number to apply the modification.\n",
        "        integration_method_applier (IntegrationMethodApplier): The method to integrate the additional input.\n",
        "        \"\"\"\n",
        "        self.model = model\n",
        "        self.layer_number = layer_number\n",
        "        self.integration_method_applier = integration_method_applier\n",
        "        self.hook = None\n",
        "\n",
        "    def modify_output(self, module, input, output):\n",
        "        \"\"\"\n",
        "        Modify the output of the specified layer.\n",
        "\n",
        "        Parameters:\n",
        "        module (nn.Module): The module to which the hook is attached.\n",
        "        input (tuple): The input to the module.\n",
        "        output (torch.Tensor): The output from the module.\n",
        "\n",
        "        Returns:\n",
        "        tuple: The modified output.\n",
        "        \"\"\"\n",
        "        output_tensor = output[0]\n",
        "        modified_output = self.integration_method_applier(output_tensor, self.additional_input_vector)\n",
        "        return (modified_output,)\n",
        "\n",
        "    def register_hook(self, additional_input_vector):\n",
        "        \"\"\"\n",
        "        Register a hook on the specified layer to modify its output.\n",
        "\n",
        "        Parameters:\n",
        "        additional_input_vector (torch.Tensor): The vector to integrate with the layer output.\n",
        "        \"\"\"\n",
        "        self.additional_input_vector = additional_input_vector\n",
        "        layer = self.model.encoder.layer[self.layer_number - 1]\n",
        "        self.hook = layer.register_forward_hook(self.modify_output)\n",
        "\n",
        "    def remove_hook(self):\n",
        "        \"\"\"\n",
        "        Remove the registered hook if it exists.\n",
        "        \"\"\"\n",
        "        if self.hook is not None:\n",
        "            self.hook.remove()\n",
        "            self.hook = None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L74Aq4AcIBb5"
      },
      "source": [
        "# Custom Layer-Based Approach\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "pmfrzbTlDF_N"
      },
      "outputs": [],
      "source": [
        "class CustomLayerBERTModifier(nn.Module):\n",
        "    def __init__(self, model, layer_number, integration_method_applier):\n",
        "        \"\"\"\n",
        "        Initialize the CustomLayerBERTModifier.\n",
        "\n",
        "        Parameters:\n",
        "        model (BertModel): The BERT model to modify.\n",
        "        layer_number (int): The layer number to start the modification.\n",
        "        integration_method_applier (IntegrationMethodApplier): The method to integrate the additional input.\n",
        "        \"\"\"\n",
        "        super(CustomLayerBERTModifier, self).__init__()\n",
        "        self.bert = model\n",
        "        self.layer_number = layer_number\n",
        "        self.integration_method_applier = integration_method_applier\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, additional_input_vector):\n",
        "        \"\"\"\n",
        "        Forward pass to apply the custom layer modification.\n",
        "\n",
        "        Parameters:\n",
        "        input_ids (torch.Tensor): The input IDs for the BERT model.\n",
        "        attention_mask (torch.Tensor): The attention mask for the BERT model.\n",
        "        additional_input_vector (torch.Tensor): The vector to integrate with the layer output.\n",
        "\n",
        "        Returns:\n",
        "        torch.Tensor: The modified output after applying the integration method.\n",
        "        \"\"\"\n",
        "        # Get the BERT model outputs with hidden states\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask, output_hidden_states=True)\n",
        "        hidden_states = outputs.hidden_states\n",
        "\n",
        "        # Apply the integration method to the specified layer's hidden states\n",
        "        modified_layer_input = self.integration_method_applier(hidden_states[self.layer_number], additional_input_vector)\n",
        "\n",
        "        # Pass the modified input through the remaining layers\n",
        "        for i in range(self.layer_number, len(self.bert.encoder.layer)):\n",
        "            modified_layer_input = self.bert.encoder.layer[i](modified_layer_input)[0]\n",
        "\n",
        "        return modified_layer_input\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "njjiAwLpfyGQ"
      },
      "source": [
        "# Compare Methods"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "WFfU2Oi2mvhQ",
        "outputId": "64701649-11bf-41e4-a695-df14ba8e2eab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hook-based method runtime: 0.084336 seconds\n",
            "Custom layer-based method runtime: 0.109123 seconds\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "def compare_methods(input_text, layer_number, integration_method):\n",
        "\n",
        "    \"\"\"\n",
        "    Compare hook-based and custom layer-based methods for modifying BERT outputs.\n",
        "\n",
        "    Parameters:\n",
        "    input_text (str): The input text to process with the BERT model.\n",
        "    layer_number (int): The layer number to apply the modification.\n",
        "    integration_method (str): The method used for integrating the additional input ('addition' or 'multiplication').\n",
        "\n",
        "    Returns:\n",
        "    bool: Whether the outputs of both methods are the same.\n",
        "    \"\"\"\n",
        "\n",
        "    # Set the seed for reproducibility\n",
        "    set_seed()\n",
        "\n",
        "    # Common setup\n",
        "    model, tokenizer = setup_model_and_tokenizer()\n",
        "    additional_input_vector = create_additional_input_vector(model.config.hidden_size)\n",
        "    integration_method_applier = IntegrationMethodApplier(integration_method)\n",
        "    inputs = prepare_inputs(input_text, tokenizer)\n",
        "\n",
        "    # Ensure the model is in evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # Hook-based modifier\n",
        "    hook_modifier = HookBasedBERTModifier(model, layer_number, integration_method_applier)\n",
        "    hook_modifier.register_hook(additional_input_vector)\n",
        "\n",
        "    start_time = time.time()\n",
        "    with torch.no_grad():\n",
        "        outputs_with_hook = model(**inputs)\n",
        "    hook_modifier.remove_hook()\n",
        "    hook_runtime = time.time() - start_time\n",
        "\n",
        "    last_hidden_state_with_hook = outputs_with_hook.last_hidden_state\n",
        "\n",
        "    # Custom layer-based modifier\n",
        "    custom_model = CustomLayerBERTModifier(model, layer_number, integration_method_applier)\n",
        "    # Ensure the custom model is in evaluation mode\n",
        "    custom_model.eval()\n",
        "\n",
        "    start_time = time.time()\n",
        "    with torch.no_grad():\n",
        "        outputs_custom = custom_model(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'], additional_input_vector=additional_input_vector)\n",
        "    custom_runtime = time.time() - start_time\n",
        "\n",
        "    last_hidden_state_custom = outputs_custom\n",
        "\n",
        "    # Compare the outputs\n",
        "    are_same = torch.allclose(last_hidden_state_with_hook, last_hidden_state_custom, atol=1e-6)\n",
        "\n",
        "    print(f\"Hook-based method runtime: {hook_runtime:.6f} seconds\")\n",
        "    print(f\"Custom layer-based method runtime: {custom_runtime:.6f} seconds\")\n",
        "    return are_same\n",
        "\n",
        "\n",
        "# Run the comparison\n",
        "compare_methods(\"Hello, how are you?\", 9, \"addition\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}